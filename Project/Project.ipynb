{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01c914ad-9011-4f06-91e5-7c3359efedc9",
   "metadata": {},
   "source": [
    "# **Project: Default of Credit Card Clients**\n",
    "\n",
    "## <font color=darkgreen>*Members*: \n",
    "> ### <font color=blue>**Chandan Patel**<br>**Rahul Bankey** <br>**Chandni Kumari**<br>**Divyanshi Singh**\n",
    "\n",
    "## Objective: \n",
    "\n",
    "### Dataset Description\n",
    "    \n",
    "### Methods and Steps followed in this notebook:\n",
    "> - #### Step 1: Importing the Data\n",
    "> - #### Step 2: Data Preprocessing\n",
    "> - #### Step 3: Data Exploration using PySpark SQL\n",
    "> - #### Step 4: Feature Engineering\n",
    "> - #### Step 5: Machine Learning Models\n",
    "    Prepare data for machine learning.<br>\n",
    "    Split into training and testing data.<br>\n",
    "    Create and evaluate Decision Trees model.<br>\n",
    "    Create and evaluate Random Forest model.<br>\n",
    "    Create and evaluate Naive Bayes model.<br>\n",
    "    Create and evaluate Multilayer Perceptron Neural Network model.\n",
    "> - #### Step 6: Clustering K-Means\n",
    "    Clustering using KMeans\n",
    "> - #### Step 7: Rest of the Machine Learning Models\n",
    "    Additional machine learning models (Decision Trees, Random Forest, Naive Bayes, MLP)\n",
    "> - #### Step 8: Model Evaluation\n",
    "    Model evaluation (accuracy, precision, recall, F1)\n",
    "> - #### Step 9: Interpreting the Best Classification\n",
    "    Displaying and interpreting the best classification model<br>\n",
    "    Displaying performance metrics<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00577aa1-0301-4d4d-b74b-f22ea9ca33b8",
   "metadata": {},
   "source": [
    "In this notebook we will use the  dataset that provides information related to the credit cards clients and store it the spark datawarehouse. We will then use the datawarehouse to access this data using spark sql.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7591d186-b8c9-4efe-81ac-6a120d2b0cac",
   "metadata": {},
   "source": [
    "## Step 1: Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24d67102-a173-4b1f-8b75-970897a5241a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import (RandomForestClassifier, \n",
    "                                      DecisionTreeClassifier, \n",
    "                                      NaiveBayes,\n",
    "                                      MultilayerPerceptronClassifier)\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1e5752-9a2f-49bf-b55e-dbfa08593ec6",
   "metadata": {},
   "source": [
    "#### Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34f5568c-fe43-44d8-a14c-e0ffcefb8157",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/07 16:13:27 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 10.21.19.148 instead (on interface eth0)\n",
      "23/11/07 16:13:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/07 16:13:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session WebUI Port: 4040\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession;\n",
    "from pyspark.context import SparkContext;\n",
    "\n",
    "# warehouse_location points to the default location for managed databases and tables\n",
    "from os.path import abspath\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "\n",
    "spark = SparkSession \\\n",
    "    . builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Big Data Analytics Project - Team Data Wizards\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc =spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\") # only display errors (not warnings)\n",
    "\n",
    "# note: If you have multiple spark sessions running (like from a previous notebook you've run), \n",
    "# this spark session webUI will be on a different port than the default (4040). One way to \n",
    "# identify this part is with the following line. If there was only one spark session running, \n",
    "# this will be 4040. If it's higher, it means there are still other spark sesssions still running.\n",
    "spark_session_port = spark.sparkContext.uiWebUrl.split(\":\")[-1]\n",
    "print(\"Spark Session WebUI Port: \" + spark_session_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e9c8173-eddc-4c71-84fb-ef81154cde96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://linux:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Big Data Analytics Project - Team Data Wizards</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7feac8378710>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fef62e5-dd0a-40cc-a35f-d50bc38c5b48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7c2675f-61ef-4f02-82f4-01f114270f45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Show theexisting  databases \n",
    "df=spark.sql(\"show databases\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdff6a2-adae-46d8-91cf-35844c43b229",
   "metadata": {},
   "source": [
    "#### List the tables in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "347dd2d2-be68-45ba-9f6e-a962bef3032c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tables = spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535ea890-8d79-477b-b4cf-40e02cc23308",
   "metadata": {},
   "source": [
    "Now, let's load the credit card dataset into the datawarehouse. We will use the spark dataframe API to load the data. We will then use the spark sql API to create a table from the dataframe. Load the file into a DataFrame and remove the header row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f84ec975-9953-4a22-af43-b2737375653e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+--------------------------+\n",
      "| ID|LIMIT_BAL|SEX|EDUCATION|MARRIAGE|AGE|PAY_0|PAY_2|PAY_3|PAY_4|PAY_5|PAY_6|BILL_AMT1|BILL_AMT2|BILL_AMT3|BILL_AMT4|BILL_AMT5|BILL_AMT6|PAY_AMT1|PAY_AMT2|PAY_AMT3|PAY_AMT4|PAY_AMT5|PAY_AMT6|default payment next month|\n",
      "+---+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+--------------------------+\n",
      "|  1|    20000|  2|        2|       1| 24|    2|    2|   -1|   -1|   -2|   -2|     3913|     3102|      689|        0|        0|        0|       0|     689|       0|       0|       0|       0|                         1|\n",
      "|  2|   120000|  2|        2|       2| 26|   -1|    2|    0|    0|    0|    2|     2682|     1725|     2682|     3272|     3455|     3261|       0|    1000|    1000|    1000|       0|    2000|                         1|\n",
      "|  3|    90000|  2|        2|       2| 34|    0|    0|    0|    0|    0|    0|    29239|    14027|    13559|    14331|    14948|    15549|    1518|    1500|    1000|    1000|    1000|    5000|                         0|\n",
      "|  4|    50000|  2|        2|       1| 37|    0|    0|    0|    0|    0|    0|    46990|    48233|    49291|    28314|    28959|    29547|    2000|    2019|    1200|    1100|    1069|    1000|                         0|\n",
      "|  5|    50000|  1|        2|       1| 57|   -1|    0|   -1|    0|    0|    0|     8617|     5670|    35835|    20940|    19146|    19131|    2000|   36681|   10000|    9000|     689|     679|                         0|\n",
      "+---+---------+---+---------+--------+---+-----+-----+-----+-----+-----+-----+---------+---------+---------+---------+---------+---------+--------+--------+--------+--------+--------+--------+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv('credit_card_clients_csv.csv', header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a919324-ad94-4823-acf4-d6a25c6a1ca0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- LIMIT_BAL: integer (nullable = true)\n",
      " |-- SEX: integer (nullable = true)\n",
      " |-- EDUCATION: integer (nullable = true)\n",
      " |-- MARRIAGE: integer (nullable = true)\n",
      " |-- AGE: integer (nullable = true)\n",
      " |-- PAY_0: integer (nullable = true)\n",
      " |-- PAY_2: integer (nullable = true)\n",
      " |-- PAY_3: integer (nullable = true)\n",
      " |-- PAY_4: integer (nullable = true)\n",
      " |-- PAY_5: integer (nullable = true)\n",
      " |-- PAY_6: integer (nullable = true)\n",
      " |-- BILL_AMT1: integer (nullable = true)\n",
      " |-- BILL_AMT2: integer (nullable = true)\n",
      " |-- BILL_AMT3: integer (nullable = true)\n",
      " |-- BILL_AMT4: integer (nullable = true)\n",
      " |-- BILL_AMT5: integer (nullable = true)\n",
      " |-- BILL_AMT6: integer (nullable = true)\n",
      " |-- PAY_AMT1: integer (nullable = true)\n",
      " |-- PAY_AMT2: integer (nullable = true)\n",
      " |-- PAY_AMT3: integer (nullable = true)\n",
      " |-- PAY_AMT4: integer (nullable = true)\n",
      " |-- PAY_AMT5: integer (nullable = true)\n",
      " |-- PAY_AMT6: integer (nullable = true)\n",
      " |-- default payment next month: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema() ## Print the schema of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c940cf2d-5e78-46e4-863a-55e14bc9320f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+--------------------+------------------+--------------------+------------------+-----------------+-----------------+-----------------+-----------------+------------------+-----------------+----------------+-----------------+------------------+-----------------+------------------+------------------+-----------------+--------------------------+\n",
      "|summary|               ID|         LIMIT_BAL|               SEX|         EDUCATION|          MARRIAGE|              AGE|             PAY_0|               PAY_2|             PAY_3|               PAY_4|             PAY_5|            PAY_6|        BILL_AMT1|        BILL_AMT2|        BILL_AMT3|         BILL_AMT4|        BILL_AMT5|       BILL_AMT6|         PAY_AMT1|          PAY_AMT2|         PAY_AMT3|          PAY_AMT4|          PAY_AMT5|         PAY_AMT6|default payment next month|\n",
      "+-------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+--------------------+------------------+--------------------+------------------+-----------------+-----------------+-----------------+-----------------+------------------+-----------------+----------------+-----------------+------------------+-----------------+------------------+------------------+-----------------+--------------------------+\n",
      "|  count|            30000|             30000|             30000|             30000|             30000|            30000|             30000|               30000|             30000|               30000|             30000|            30000|            30000|            30000|            30000|             30000|            30000|           30000|            30000|             30000|            30000|             30000|             30000|            30000|                     30000|\n",
      "|   mean|          15000.5|167484.32266666667|1.6037333333333332|1.8531333333333333|1.5518666666666667|          35.4855|           -0.0167|-0.13376666666666667|           -0.1662|-0.22066666666666668|           -0.2662|          -0.2911|       51223.3309|49179.07516666667|       47013.1548| 43262.94896666666|40311.40096666667|      38871.7604|        5663.5805|         5921.1635|        5225.6815| 4826.076866666666| 4799.387633333334|5215.502566666667|                    0.2212|\n",
      "| stddev|8660.398374208891|129747.66156720246|0.4891291960902602|0.7903486597207269|0.5219696006132467|9.217904068090155|1.1238015279973335|  1.1971859730345495|1.1968675684465686|  1.1691386224023357|1.1331874060027525|1.149987625607897|73635.86057552966|71173.76878252832|69349.38742703677|64332.856133916444|60797.15577026471|59554.1075367459|16563.28035402577|23040.870402057186|17606.96146980311|15666.159744032062|15278.305679144742|17777.46577543531|       0.41506180569093254|\n",
      "|    min|                1|             10000|                 1|                 0|                 0|               21|                -2|                  -2|                -2|                  -2|                -2|               -2|          -165580|           -69777|          -157264|           -170000|           -81334|         -339603|                0|                 0|                0|                 0|                 0|                0|                         0|\n",
      "|    max|            30000|           1000000|                 2|                 6|                 3|               79|                 8|                   8|                 8|                   8|                 8|                8|           964511|           983931|          1664089|            891586|           927171|          961664|           873552|           1684259|           896040|            621000|            426529|           528666|                         1|\n",
      "+-------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+--------------------+------------------+--------------------+------------------+-----------------+-----------------+-----------------+-----------------+------------------+-----------------+----------------+-----------------+------------------+-----------------+------------------+------------------+-----------------+--------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Check the summary statistics and descriptions of numeric columns\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d672812f-c083-4917-99cc-d96c30be832b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aef55c68-f23d-440d-afdd-ed7f2eb2f8fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[NOT_COLUMN_OR_STR] Argument `condition` should be a Column or str, got function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### remove unncessary row\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_raw \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28;01mlambda\u001b[39;00m z: EDUCATION \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m z)\n\u001b[1;32m      3\u001b[0m df_raw\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m      4\u001b[0m df_raw\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/bd/lib/python3.11/site-packages/pyspark/sql/dataframe.py:3327\u001b[0m, in \u001b[0;36mDataFrame.filter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   3325\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mfilter(condition\u001b[38;5;241m.\u001b[39m_jc)\n\u001b[1;32m   3326\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   3328\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN_OR_STR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3329\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcondition\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(condition)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   3330\u001b[0m     )\n\u001b[1;32m   3331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [NOT_COLUMN_OR_STR] Argument `condition` should be a Column or str, got function."
     ]
    }
   ],
   "source": [
    " ### remove unncessary row\n",
    "df_raw = df.filter(lambda z: \"EDUCATION\" not in z)\n",
    "df_raw.count()\n",
    "df_raw.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22c00c07-f2ed-4f24-8c19-f2f880c3fe4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `value` cannot be resolved. Did you mean one of the following? [`AGE`, `ID`, `SEX`, `PAY_0`, `PAY_2`].;\n'Project [ID#295, LIMIT_BAL#296, SEX#297, EDUCATION#298, MARRIAGE#299, AGE#300, PAY_0#301, PAY_2#302, PAY_3#303, PAY_4#304, PAY_5#305, PAY_6#306, BILL_AMT1#307, BILL_AMT2#308, BILL_AMT3#309, BILL_AMT4#310, BILL_AMT5#311, BILL_AMT6#312, PAY_AMT1#313, PAY_AMT2#314, PAY_AMT3#315, PAY_AMT4#316, PAY_AMT5#317, PAY_AMT6#318, ... 2 more fields]\n+- Relation [ID#295,LIMIT_BAL#296,SEX#297,EDUCATION#298,MARRIAGE#299,AGE#300,PAY_0#301,PAY_2#302,PAY_3#303,PAY_4#304,PAY_5#305,PAY_6#306,BILL_AMT1#307,BILL_AMT2#308,BILL_AMT3#309,BILL_AMT4#310,BILL_AMT5#311,BILL_AMT6#312,PAY_AMT1#313,PAY_AMT2#314,PAY_AMT3#315,PAY_AMT4#316,PAY_AMT5#317,PAY_AMT6#318,default payment next month#319] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Remove double quotes from records\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_raw \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      3\u001b[0m df_raw \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdouble\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/bd/lib/python3.11/site-packages/pyspark/sql/dataframe.py:5170\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   5165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[1;32m   5166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   5167\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5168\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   5169\u001b[0m     )\n\u001b[0;32m-> 5170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mwithColumn(colName, col\u001b[38;5;241m.\u001b[39m_jc), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/miniconda3/envs/bd/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/bd/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `value` cannot be resolved. Did you mean one of the following? [`AGE`, `ID`, `SEX`, `PAY_0`, `PAY_2`].;\n'Project [ID#295, LIMIT_BAL#296, SEX#297, EDUCATION#298, MARRIAGE#299, AGE#300, PAY_0#301, PAY_2#302, PAY_3#303, PAY_4#304, PAY_5#305, PAY_6#306, BILL_AMT1#307, BILL_AMT2#308, BILL_AMT3#309, BILL_AMT4#310, BILL_AMT5#311, BILL_AMT6#312, PAY_AMT1#313, PAY_AMT2#314, PAY_AMT3#315, PAY_AMT4#316, PAY_AMT5#317, PAY_AMT6#318, ... 2 more fields]\n+- Relation [ID#295,LIMIT_BAL#296,SEX#297,EDUCATION#298,MARRIAGE#299,AGE#300,PAY_0#301,PAY_2#302,PAY_3#303,PAY_4#304,PAY_5#305,PAY_6#306,BILL_AMT1#307,BILL_AMT2#308,BILL_AMT3#309,BILL_AMT4#310,BILL_AMT5#311,BILL_AMT6#312,PAY_AMT1#313,PAY_AMT2#314,PAY_AMT3#315,PAY_AMT4#316,PAY_AMT5#317,PAY_AMT6#318,default payment next month#319] csv\n"
     ]
    }
   ],
   "source": [
    "# Remove double quotes from records\n",
    "df_raw = df.withColumn(\"value\", col(\"value\").cast(\"string\"))\n",
    "df_raw = df.withColumn(\"value\", col(\"value\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f179196-0139-4b32-bcd2-4dde6e96507e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Normalize sex to 1 and 2\n",
    "df_raw = df.withColumn(\"SEX\", (col(\"SEX\") == \"F\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46981ff6-6f45-477b-84a8-f3c05423b456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9e8981-47b4-4290-a376-f777e6cc3b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7707be71-45fb-4b3b-8da4-bd7baf97b6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ID,LIMIT_BAL,SEX,EDUCATION,MARRIAGE,AGE,PAY_0,PAY_2,PAY_3,PAY_4,PAY_5,PAY_6,BILL_AMT1,BILL_AMT2,BILL_AMT3,BILL_AMT4,BILL_AMT5,BILL_AMT6,PAY_AMT1,PAY_AMT2,PAY_AMT3,PAY_AMT4,PAY_AMT5,PAY_AMT6,default payment next month',\n",
       " '1,20000,2,2,1,24,2,2,-1,-1,-2,-2,3913,3102,689,0,0,0,0,689,0,0,0,0,1',\n",
       " '2,120000,2,2,2,26,-1,2,0,0,0,2,2682,1725,2682,3272,3455,3261,0,1000,1000,1000,0,2000,1',\n",
       " '3,90000,2,2,2,34,0,0,0,0,0,0,29239,14027,13559,14331,14948,15549,1518,1500,1000,1000,1000,5000,0',\n",
       " '4,50000,2,2,1,37,0,0,0,0,0,0,46990,48233,49291,28314,28959,29547,2000,2019,1200,1100,1069,1000,0']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the file into a RDD\n",
    "ccRaw = spark.sparkContext.textFile(\"credit_card_clients_csv.csv\")\n",
    "ccRaw.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd23c79d-043e-4338-8f8a-f071f1637ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Remove header row\n",
    "dataLines = ccRaw.filter(lambda z: \"EDUCATION\" not in z)\n",
    "dataLines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef4d592a-6753-4466-b5ee-6bc09a15f0b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,20000,2,2,1,24,2,2,-1,-1,-2,-2,3913,3102,689,0,0,0,0,689,0,0,0,0,1',\n",
       " '2,120000,2,2,2,26,-1,2,0,0,0,2,2682,1725,2682,3272,3455,3261,0,1000,1000,1000,0,2000,1',\n",
       " '3,90000,2,2,2,34,0,0,0,0,0,0,29239,14027,13559,14331,14948,15549,1518,1500,1000,1000,1000,5000,0',\n",
       " '4,50000,2,2,1,37,0,0,0,0,0,0,46990,48233,49291,28314,28959,29547,2000,2019,1200,1100,1069,1000,0',\n",
       " '5,50000,1,2,1,57,-1,0,-1,0,0,0,8617,5670,35835,20940,19146,19131,2000,36681,10000,9000,689,679,0']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataLines.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "034823f3-6b96-412c-9ff7-015aa0e49a9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove double quotes that are present in few records.\n",
    "cleanedLines = dataLines.map(lambda x: x.replace(\"\\\"\", \"\"))\n",
    "cleanedLines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36f7fb51-aba0-46fa-a7ef-baa4912d0e34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[27] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedLines.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ff00289-9eff-420b-b2e9-667627993468",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convertToRow(instr) :\n",
    "    attributeList = instr.split(\",\")\n",
    " \n",
    "    # rounding of age to range of 10s.    \n",
    "    ageRound = round(float(attributeList[5]) / 10.0) * 10\n",
    "    \n",
    "    #Normalize sex to only 1 and 2.\n",
    "    sex = attributeList[2]\n",
    "    if sex ==\"M\":\n",
    "        sex=1\n",
    "    elif sex == \"F\":\n",
    "        sex=2\n",
    "    \n",
    "    #average billed Amount.\n",
    "    avgBillAmt = (float(attributeList[12]) +  \\\n",
    "                    float(attributeList[13]) + \\\n",
    "                    float(attributeList[15]) + \\\n",
    "                    float(attributeList[16]) + \\\n",
    "                    float(attributeList[16]) + \\\n",
    "                    float(attributeList[17]) ) / 6.0\n",
    "                    \n",
    "    #average pay amount\n",
    "    avgPayAmt = (float(attributeList[18]) +  \\\n",
    "                    float(attributeList[19]) + \\\n",
    "                    float(attributeList[20]) + \\\n",
    "                    float(attributeList[21]) + \\\n",
    "                    float(attributeList[22]) + \\\n",
    "                    float(attributeList[23]) ) / 6.0\n",
    "                    \n",
    "    #Find average pay duration. \n",
    "    #Make sure numbers are rounded and negative values are eliminated\n",
    "    avgPayDuration = round((abs(float(attributeList[6])) + \\\n",
    "                        abs(float(attributeList[7])) + \\\n",
    "                        abs(float(attributeList[8])) +\\\n",
    "                        abs(float(attributeList[9])) +\\\n",
    "                        abs(float(attributeList[10])) +\\\n",
    "                        abs(float(attributeList[11]))) / 6)\n",
    "    \n",
    "    #Average percentage paid. add this as an additional field to see\n",
    "    #if this field has any predictive capabilities. This is \n",
    "    #additional creative work that you do to see possibilities.                    \n",
    "    perPay = round((avgPayAmt/(avgBillAmt+1) * 100) / 25) * 25\n",
    "                    \n",
    "    values = Row (  CUSTID = attributeList[0], \\\n",
    "                    LIMIT_BAL = float(attributeList[1]), \\\n",
    "                    SEX = float(sex),\\\n",
    "                    EDUCATION = float(attributeList[3]),\\\n",
    "                    MARRIAGE = float(attributeList[4]),\\\n",
    "                    AGE = float(ageRound), \\\n",
    "                    AVG_PAY_DUR = float(avgPayDuration),\\\n",
    "                    AVG_BILL_AMT = abs(float(avgBillAmt)), \\\n",
    "                    AVG_PAY_AMT = float(avgPayAmt), \\\n",
    "                    PER_PAID= abs(float(perPay)), \\\n",
    "                    DEFAULTED = float(attributeList[24]) \n",
    "                    )\n",
    "\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba5670bf-a8b0-4d23-87c0-960160b1b968",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(CUSTID='1', LIMIT_BAL=20000.0, SEX=2.0, EDUCATION=2.0, MARRIAGE=1.0, AGE=20.0, AVG_PAY_DUR=2.0, AVG_BILL_AMT=1169.1666666666667, AVG_PAY_AMT=114.83333333333333, PER_PAID=0.0, DEFAULTED=1.0),\n",
       " Row(CUSTID='2', LIMIT_BAL=120000.0, SEX=2.0, EDUCATION=2.0, MARRIAGE=2.0, AGE=30.0, AVG_PAY_DUR=1.0, AVG_BILL_AMT=2975.0, AVG_PAY_AMT=833.3333333333334, PER_PAID=25.0, DEFAULTED=1.0),\n",
       " Row(CUSTID='3', LIMIT_BAL=90000.0, SEX=2.0, EDUCATION=2.0, MARRIAGE=2.0, AGE=30.0, AVG_PAY_DUR=0.0, AVG_BILL_AMT=17173.666666666668, AVG_PAY_AMT=1836.3333333333333, PER_PAID=0.0, DEFAULTED=0.0),\n",
       " Row(CUSTID='4', LIMIT_BAL=50000.0, SEX=2.0, EDUCATION=2.0, MARRIAGE=1.0, AGE=40.0, AVG_PAY_DUR=0.0, AVG_BILL_AMT=35167.0, AVG_PAY_AMT=1398.0, PER_PAID=0.0, DEFAULTED=0.0),\n",
       " Row(CUSTID='5', LIMIT_BAL=50000.0, SEX=1.0, EDUCATION=2.0, MARRIAGE=1.0, AGE=60.0, AVG_PAY_DUR=0.0, AVG_BILL_AMT=15441.666666666666, AVG_PAY_AMT=9841.5, PER_PAID=75.0, DEFAULTED=0.0)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cleanedup RDD    \n",
    "ccRows = cleanedLines.map(convertToRow)\n",
    "ccRows.take(5)\n",
    "# ccRows = df.map(convertToRow)\n",
    "# ccRows.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "068a7ac0-ea88-4742-9e42-13bac6685adf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---+---------+--------+----+-----------+------------------+------------------+--------+---------+\n",
      "|CUSTID|LIMIT_BAL|SEX|EDUCATION|MARRIAGE| AGE|AVG_PAY_DUR|      AVG_BILL_AMT|       AVG_PAY_AMT|PER_PAID|DEFAULTED|\n",
      "+------+---------+---+---------+--------+----+-----------+------------------+------------------+--------+---------+\n",
      "|     1|  20000.0|2.0|      2.0|     1.0|20.0|        2.0|1169.1666666666667|114.83333333333333|     0.0|      1.0|\n",
      "|     2| 120000.0|2.0|      2.0|     2.0|30.0|        1.0|            2975.0| 833.3333333333334|    25.0|      1.0|\n",
      "|     3|  90000.0|2.0|      2.0|     2.0|30.0|        0.0|17173.666666666668|1836.3333333333333|     0.0|      0.0|\n",
      "|     4|  50000.0|2.0|      2.0|     1.0|40.0|        0.0|           35167.0|            1398.0|     0.0|      0.0|\n",
      "|     5|  50000.0|1.0|      2.0|     1.0|60.0|        0.0|15441.666666666666|            9841.5|    75.0|      0.0|\n",
      "+------+---------+---+---------+--------+----+-----------+------------------+------------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create a data frame.\n",
    "ccDf = spark.createDataFrame(ccRows)\n",
    "#ccDf.cache()\n",
    "ccDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa715fc7-4309-4833-9513-8e8adb9cab9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CUSTID: string (nullable = true)\n",
      " |-- LIMIT_BAL: double (nullable = true)\n",
      " |-- SEX: double (nullable = true)\n",
      " |-- EDUCATION: double (nullable = true)\n",
      " |-- MARRIAGE: double (nullable = true)\n",
      " |-- AGE: double (nullable = true)\n",
      " |-- AVG_PAY_DUR: double (nullable = true)\n",
      " |-- AVG_BILL_AMT: double (nullable = true)\n",
      " |-- AVG_PAY_AMT: double (nullable = true)\n",
      " |-- PER_PAID: double (nullable = true)\n",
      " |-- DEFAULTED: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ccDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a47b8848-18ac-42e9-a0cf-253edc225e5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Enhance Data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f00a0ad8-9ddf-4dc3-a731-705259e95120",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#Add SEXNAME to the data using SQL Joins.\n",
    "genderDict = [{\"SEX\" : 1.0, \"SEX_NAME\" : \"Male\"}, \\\n",
    "                {\"SEX\" : 2.0, \"SEX_NAME\" : \"Female\"}]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e5e02ed-1b5f-43b8-b949-a94f3a4ec163",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "genderDf = spark.createDataFrame(pd.DataFrame(genderDict, \\\n",
    "            columns=['SEX', 'SEX_NAME']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c7f2413-ff9e-4925-8a1f-b510995b26f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(SEX=1.0, SEX_NAME='Male'), Row(SEX=2.0, SEX_NAME='Female')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genderDf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "457e5028-036f-4a93-b7d6-703d5fb5b9d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CUSTID: string (nullable = true)\n",
      " |-- LIMIT_BAL: double (nullable = true)\n",
      " |-- SEX: double (nullable = true)\n",
      " |-- EDUCATION: double (nullable = true)\n",
      " |-- MARRIAGE: double (nullable = true)\n",
      " |-- AGE: double (nullable = true)\n",
      " |-- AVG_PAY_DUR: double (nullable = true)\n",
      " |-- AVG_BILL_AMT: double (nullable = true)\n",
      " |-- AVG_PAY_AMT: double (nullable = true)\n",
      " |-- PER_PAID: double (nullable = true)\n",
      " |-- DEFAULTED: double (nullable = true)\n",
      " |-- SEX_NAME: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ccDf1 = ccDf.join( genderDf, ccDf.SEX== genderDf.SEX ).drop(genderDf.SEX)\n",
    "ccDf1.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4781e6ed-2e4c-4ac3-8319-83a0de34b44e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Add ED_STR to the data with SQL joins.\n",
    "eduDict = [{\"EDUCATION\" : 1.0, \"ED_STR\" : \"Graduate\"}, \\\n",
    "                {\"EDUCATION\" : 2.0, \"ED_STR\" : \"University\"}, \\\n",
    "                {\"EDUCATION\" : 3.0, \"ED_STR\" : \"High School\" }, \\\n",
    "                {\"EDUCATION\" : 4.0, \"ED_STR\" : \"Others\"}]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e493c952-72dc-4864-9766-9ba14585880b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eduDf = spark.createDataFrame(pd.DataFrame(eduDict, \\\n",
    "            columns=['EDUCATION', 'ED_STR']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "031e8ffe-0e78-4ec6-83cc-f65262ddd1e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(EDUCATION=1.0, ED_STR='Graduate'),\n",
       " Row(EDUCATION=2.0, ED_STR='University'),\n",
       " Row(EDUCATION=3.0, ED_STR='High School'),\n",
       " Row(EDUCATION=4.0, ED_STR='Others')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eduDf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8b79e918-6000-42da-8287-f02f73cf3cdb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CUSTID: string (nullable = true)\n",
      " |-- LIMIT_BAL: double (nullable = true)\n",
      " |-- SEX: double (nullable = true)\n",
      " |-- EDUCATION: double (nullable = true)\n",
      " |-- MARRIAGE: double (nullable = true)\n",
      " |-- AGE: double (nullable = true)\n",
      " |-- AVG_PAY_DUR: double (nullable = true)\n",
      " |-- AVG_BILL_AMT: double (nullable = true)\n",
      " |-- AVG_PAY_AMT: double (nullable = true)\n",
      " |-- PER_PAID: double (nullable = true)\n",
      " |-- DEFAULTED: double (nullable = true)\n",
      " |-- SEX_NAME: string (nullable = true)\n",
      " |-- ED_STR: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ccDf2 = ccDf1.join( eduDf, ccDf1.EDUCATION== eduDf.EDUCATION ).drop(eduDf.EDUCATION)\n",
    "ccDf2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "925f8c2c-c2e9-40ef-9f1a-07995f99b52b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#Add MARR_DESC to the data. Required for PR#03\n",
    "marrDict = [{\"MARRIAGE\" : 1.0, \"MARR_DESC\" : \"Single\"}, \\\n",
    "                {\"MARRIAGE\" : 2.0, \"MARR_DESC\" : \"Married\"}, \\\n",
    "                {\"MARRIAGE\" : 3.0, \"MARR_DESC\" : \"Others\"}]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a4cd35a5-cddb-4a4e-ae0d-dbbca610cfbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "marrDf = spark.createDataFrame(pd.DataFrame(marrDict, \\\n",
    "            columns=['MARRIAGE', 'MARR_DESC']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "31a6b06c-60c8-4972-9944-d3405f38c67a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(MARRIAGE=1.0, MARR_DESC='Single'),\n",
       " Row(MARRIAGE=2.0, MARR_DESC='Married'),\n",
       " Row(MARRIAGE=3.0, MARR_DESC='Others')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marrDf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4c78c1d4-2a60-4c1c-840b-55a91f5e7880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ccFinalDf = ccDf2.join( marrDf, ccDf2.MARRIAGE== marrDf.MARRIAGE ).drop(marrDf.MARRIAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "13b6763f-9fd4-4102-8a04-022d9cfd73fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[CUSTID: string, LIMIT_BAL: double, SEX: double, EDUCATION: double, MARRIAGE: double, AGE: double, AVG_PAY_DUR: double, AVG_BILL_AMT: double, AVG_PAY_AMT: double, PER_PAID: double, DEFAULTED: double, SEX_NAME: string, ED_STR: string, MARR_DESC: string]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ccFinalDf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "387778ba-e1c1-4aee-8b35-5bb00f03a64e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/07 16:39:57 ERROR Executor: Exception in task 0.0 in stage 31.0 (TID 236)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_103094/1815832823.py\", line 42, in convertToRow\n",
      "ZeroDivisionError: float division by zero\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/11/07 16:39:57 ERROR TaskSetManager: Task 0 in stage 31.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o552.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 31.0 failed 1 times, most recent failure: Lost task 0.0 in stage 31.0 (TID 236) (linux executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_103094/1815832823.py\", line 42, in convertToRow\nZeroDivisionError: float division by zero\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_103094/1815832823.py\", line 42, in convertToRow\nZeroDivisionError: float division by zero\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ccFinalDf\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/bd/lib/python3.11/site-packages/pyspark/sql/dataframe.py:959\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    954\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    955\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    956\u001b[0m     )\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 959\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;241m20\u001b[39m, vertical))\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/bd/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/bd/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/miniconda3/envs/bd/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o552.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 31.0 failed 1 times, most recent failure: Lost task 0.0 in stage 31.0 (TID 236) (linux executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_103094/1815832823.py\", line 42, in convertToRow\nZeroDivisionError: float division by zero\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/student/miniconda3/envs/bd/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_103094/1815832823.py\", line 42, in convertToRow\nZeroDivisionError: float division by zero\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:168)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "ccFinalDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d4717b-14de-43b4-bf4f-b95184536d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
